{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhiopoQoJ0OeVg3eLHRTN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayema32/Pytorch/blob/main/C1_w4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controllable Generation"
      ],
      "metadata": {
        "id": "yfkEVr4H9qU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages and Visualization"
      ],
      "metadata": {
        "id": "deaQ2UwG-BnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyP8FeCf9n5y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import CelebA\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator and Noise"
      ],
      "metadata": {
        "id": "Vxt-ute3I71i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (CelebA is rgb, so 3 is our default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
        "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise\n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor,\n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples in the batch, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, z_dim, device=device)"
      ],
      "metadata": {
        "id": "_pyhknu1I9GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier"
      ],
      "metadata": {
        "id": "RiUsnXviJE0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    '''\n",
        "    Classifier Class\n",
        "    Values:\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (CelebA is rgb, so 3 is our default)\n",
        "        n_classes: the total number of classes in the dataset, an integer scalar\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, im_chan=3, n_classes=2, hidden_dim=64):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            self.make_classifier_block(im_chan, hidden_dim),\n",
        "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
        "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_classifier_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a classifier block;\n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise\n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the classifier: Given an image tensor,\n",
        "        returns an n_classes-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with im_chan channels\n",
        "        '''\n",
        "        class_pred = self.classifier(image)\n",
        "        return class_pred.view(len(class_pred), -1)"
      ],
      "metadata": {
        "id": "jDU-LgQpJGO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying Parameters"
      ],
      "metadata": {
        "id": "fpqHNw6iJNb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 64\n",
        "batch_size = 128\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "qea-KZB1JOsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Classifier"
      ],
      "metadata": {
        "id": "B6n8L9IsJWmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(filename):\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # You can run this code to train your own classifier, but there is a provided pretrained one.\n",
        "    # If you'd like to use this, just run \"train_classifier(filename)\"\n",
        "    # to train and save a classifier on the label indices to that filename.\n",
        "\n",
        "    # Target all the classes, so that's how many the classifier will learn\n",
        "    label_indices = range(40)\n",
        "\n",
        "    n_epochs = 3\n",
        "    display_step = 500\n",
        "    lr = 0.001\n",
        "    beta_1 = 0.5\n",
        "    beta_2 = 0.999\n",
        "    image_size = 64\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        CelebA(\".\", split='train', download=True, transform=transform),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True)\n",
        "\n",
        "    classifier = Classifier(n_classes=len(label_indices)).to(device)\n",
        "    class_opt = torch.optim.Adam(classifier.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    cur_step = 0\n",
        "    classifier_losses = []\n",
        "    # classifier_val_losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        # Dataloader returns the batches\n",
        "        for real, labels in tqdm(dataloader):\n",
        "            real = real.to(device)\n",
        "            labels = labels[:, label_indices].to(device).float()\n",
        "\n",
        "            class_opt.zero_grad()\n",
        "            class_pred = classifier(real)\n",
        "            class_loss = criterion(class_pred, labels)\n",
        "            class_loss.backward() # Calculate the gradients\n",
        "            class_opt.step() # Update the weights\n",
        "            classifier_losses += [class_loss.item()] # Keep track of the average classifier loss\n",
        "\n",
        "            ## Visualization code ##\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                class_mean = sum(classifier_losses[-display_step:]) / display_step\n",
        "                print(f\"Step {cur_step}: Classifier loss: {class_mean}\")\n",
        "                step_bins = 20\n",
        "                x_axis = sorted([i * step_bins for i in range(len(classifier_losses) // step_bins)] * step_bins)\n",
        "                sns.lineplot(x_axis, classifier_losses[:len(x_axis)], label=\"Classifier Loss\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "                torch.save({\"classifier\": classifier.state_dict()}, filename)\n",
        "            cur_step += 1\n",
        "\n",
        "# Uncomment the last line to train your own classfier - this line will not work in Coursera.\n",
        "# If you'd like to do this, you'll have to download it and run it, ideally using a GPU\n",
        "# train_classifier(\"filename\")"
      ],
      "metadata": {
        "id": "0Ol34x9TJXww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Pretrained Models"
      ],
      "metadata": {
        "id": "mWlYQV81JpC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Assuming Generator and Classifier classes are defined elsewhere in your code\n",
        "# Ensure the necessary imports and definitions\n",
        "\n",
        "# Define the Generator class\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
        "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "# Define the Classifier class\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, im_chan=3, n_classes=40, hidden_dim=64):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            self.make_classifier_block(im_chan, hidden_dim),\n",
        "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
        "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_classifier_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        if final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        class_pred = self.classifier(image)\n",
        "        return class_pred.view(len(class_pred), -1)\n",
        "\n",
        "# Ensure device is set\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set the file paths\n",
        "gen_path = \"pretrained_celeba.pth\"\n",
        "classifier_path = \"pretrained_classifier.pth\"\n",
        "\n",
        "# Check if the generator model file exists\n",
        "if not os.path.isfile(gen_path):\n",
        "    raise FileNotFoundError(f\"Generator model file not found: {gen_path}\")\n",
        "\n",
        "# Check if the classifier model file exists\n",
        "if not os.path.isfile(classifier_path):\n",
        "    raise FileNotFoundError(f\"Classifier model file not found: {classifier_path}\")\n",
        "\n",
        "# Load Generator model\n",
        "z_dim = 64  # Ensure z_dim is set correctly\n",
        "gen = Generator(z_dim=z_dim).to(device)\n",
        "gen_dict = torch.load(gen_path, map_location=device)[\"gen\"]\n",
        "gen.load_state_dict(gen_dict)\n",
        "gen.eval()\n",
        "\n",
        "# Load Classifier model\n",
        "n_classes = 40\n",
        "classifier = Classifier(n_classes=n_classes).to(device)\n",
        "class_dict = torch.load(classifier_path, map_location=device)[\"classifier\"]\n",
        "classifier.load_state_dict(class_dict)\n",
        "classifier.eval()\n",
        "\n",
        "print(\"Loaded the models!\")\n",
        "\n",
        "# Set up the optimizer\n",
        "opt = torch.optim.Adam(classifier.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aN9kZ4zNJqdI",
        "outputId": "9e8ba073-eef9-4f1c-c269-12681314808e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Generator model file not found: pretrained_celeba.pth",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cba99b24dfb6>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Check if the generator model file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generator model file not found: {gen_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Check if the classifier model file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Generator model file not found: pretrained_celeba.pth"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "calculate_updated_noise"
      ],
      "metadata": {
        "id": "d3S0oXtUJxTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: calculate_updated_noise\n",
        "def calculate_updated_noise(noise, weight):\n",
        "    '''\n",
        "    Function to return noise vectors updated with stochastic gradient ascent.\n",
        "    Parameters:\n",
        "        noise: the current noise vectors. You have already called the backwards function on the target class\n",
        "          so you can access the gradient of the output class with respect to the noise by using noise.grad\n",
        "        weight: the scalar amount by which you should weight the noise gradient\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    new_noise = noise + ( noise.grad * weight)\n",
        "    #### END CODE HERE ####\n",
        "    return new_noise"
      ],
      "metadata": {
        "id": "gmr_djrmJ3LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "# Check that the basic function works\n",
        "opt.zero_grad()\n",
        "noise = torch.ones(20, 20) * 2\n",
        "noise.requires_grad_()\n",
        "fake_classes = (noise ** 2).mean()\n",
        "fake_classes.backward()\n",
        "new_noise = calculate_updated_noise(noise, 0.1)\n",
        "assert type(new_noise) == torch.Tensor\n",
        "assert tuple(new_noise.shape) == (20, 20)\n",
        "assert new_noise.max() == 2.0010\n",
        "assert new_noise.min() == 2.0010\n",
        "assert torch.isclose(new_noise.sum(), torch.tensor(0.4) + 20 * 20 * 2)\n",
        "print(\"Success!\")"
      ],
      "metadata": {
        "id": "F7WBs00fJ-RG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9fcafe8d-0f73-47a8-c6f7-67f393da5e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'opt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9ae96d3dbfd0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# UNIT TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Check that the basic function works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that it works for generated images\n",
        "opt.zero_grad()\n",
        "noise = get_noise(32, z_dim).to(device).requires_grad_()\n",
        "fake = gen(noise)\n",
        "fake_classes = classifier(fake)[:, 0]\n",
        "fake_classes.mean().backward()\n",
        "noise.data = calculate_updated_noise(noise, 0.01)\n",
        "fake = gen(noise)\n",
        "fake_classes_new = classifier(fake)[:, 0]\n",
        "assert torch.all(fake_classes_new > fake_classes)\n",
        "print(\"Success!\")\n"
      ],
      "metadata": {
        "id": "8C6n5vpsJ_z_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "30ac1f74-61ad-4445-8979-11fd1f4c58bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'opt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-741b0534acd7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check that it works for generated images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfake_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation"
      ],
      "metadata": {
        "id": "xeB7X-suKJyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First generate a bunch of images with the generator\n",
        "n_images = 8\n",
        "fake_image_history = []\n",
        "grad_steps = 10 # Number of gradient steps to take\n",
        "skip = 2 # Number of gradient steps to skip in the visualization\n",
        "\n",
        "feature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n",
        "\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n",
        "\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\",\n",
        "\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\",\n",
        "\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\",\n",
        "\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n",
        "\n",
        "### Change me! ###\n",
        "target_indices = feature_names.index(\"Smiling\") # Feel free to change this value to any string from feature_names!\n",
        "\n",
        "noise = get_noise(n_images, z_dim).to(device).requires_grad_()\n",
        "for i in range(grad_steps):\n",
        "    opt.zero_grad()\n",
        "    fake = gen(noise)\n",
        "    fake_image_history += [fake]\n",
        "    fake_classes_score = classifier(fake)[:, target_indices].mean()\n",
        "    fake_classes_score.backward()\n",
        "    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\n",
        "show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_imag"
      ],
      "metadata": {
        "id": "-Sa0svpPKLRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "60ccbdec-96d2-4f89-8d90-8c5c04635a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-29-41ed5c3beb22>, line 27)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-41ed5c3beb22>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_imag\u001b[0m\n\u001b[0m                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entanglement and Regularization"
      ],
      "metadata": {
        "id": "y4gK3PYIKQ5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_score"
      ],
      "metadata": {
        "id": "2oAG2ykeKVco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_score\n",
        "def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n",
        "    '''\n",
        "    Function to return the score of the current classifications, penalizing changes\n",
        "    to other classes with an L2 norm.\n",
        "    Parameters:\n",
        "        current_classifications: the classifications associated with the current noise\n",
        "        original_classifications: the classifications associated with the original noise\n",
        "        target_indices: the index of the target class\n",
        "        other_indices: the indices of the other classes\n",
        "        penalty_weight: the amount that the penalty should be weighted in the overall score\n",
        "    '''\n",
        "    # Steps: 1) Calculate the change between the original and current classifications (as a tensor)\n",
        "    #           by indexing into the other_indices you're trying to preserve, like in x[:, features].\n",
        "    #        2) Calculate the norm (magnitude) of changes per example.\n",
        "    #        3) Multiply the mean of the example norms by the penalty weight.\n",
        "    #           This will be your other_class_penalty.\n",
        "    #           Make sure to negate the value since it's a penalty!\n",
        "    #        4) Take the mean of the current classifications for the target feature over all the examples.\n",
        "    #           This mean will be your target_score.\n",
        "    #### START CODE HERE ####\n",
        "    other_distances = current_classifications[:,other_indices] - original_classifications[:,other_indices]\n",
        "    # Calculate the norm (magnitude) of changes per example and multiply by penalty weight\n",
        "    other_class_penalty = -torch.norm(other_distances, dim=1).mean() * penalty_weight\n",
        "    # Take the mean of the current classifications for the target feature\n",
        "    target_score = current_classifications[:, target_indices].mean()\n",
        "    #### END CODE HERE ####\n",
        "    return target_score + other_class_penalty"
      ],
      "metadata": {
        "id": "-6FCqikhKRsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "assert torch.isclose(\n",
        "    get_score(torch.ones(4, 3), torch.zeros(4, 3), [0], [1, 2], 0.2),\n",
        "    1 - torch.sqrt(torch.tensor(2.)) * 0.2\n",
        ")\n",
        "rows = 10\n",
        "current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
        "original_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
        "\n",
        "# Must be 3\n",
        "assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3\n",
        "\n",
        "current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
        "original_class = torch.tensor([[4] * rows, [4] * rows, [2] * rows, [1] * rows]).T.float()\n",
        "\n",
        "# Must be 3 - 0.2 * sqrt(10)\n",
        "assert torch.isclose(get_score(current_class, original_class, [1, 3] , [0, 2], 0.2),\n",
        "                     -torch.sqrt(torch.tensor(10.0)) * 0.2 + 3)\n",
        "\n",
        "print(\"Success!\")"
      ],
      "metadata": {
        "id": "lWQg-aehKckd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_image_history = []\n",
        "### Change me! ###\n",
        "target_indices = feature_names.index(\"Smiling\") # Feel free to change this value to any string from feature_names from earlier!\n",
        "other_indices = [cur_idx != target_indices for cur_idx, _ in enumerate(feature_names)]\n",
        "noise = get_noise(n_images, z_dim).to(device).requires_grad_()\n",
        "original_classifications = classifier(gen(noise)).detach()\n",
        "for i in range(grad_steps):\n",
        "    opt.zero_grad()\n",
        "    fake = gen(noise)\n",
        "    fake_image_history += [fake]\n",
        "    fake_score = get_score(\n",
        "        classifier(fake),\n",
        "        original_classifications,\n",
        "        target_indices,\n",
        "        other_indices,\n",
        "        penalty_weight=0.1\n",
        "    )\n",
        "    fake_score.backward()\n",
        "    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\n",
        "show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_images)"
      ],
      "metadata": {
        "id": "KRGjB5CoKkvJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}